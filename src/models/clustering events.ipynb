{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45dec303-119e-4216-aa2c-a6aab1274c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['en_core_web_md']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "print(spacy.util.get_installed_models())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9b97408-4c5c-4fe3-aad5-d66acfe84528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ No changes detected. Waiting...\n",
      "\n",
      "ðŸ”„ Detected change in all_processed_posts.json. Processing new posts...\n",
      "\n",
      "ðŸ”¹ Found 0 new unclustered posts.\n",
      "ðŸ”¹ No new posts to process. Waiting for updates...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "\n",
      "ðŸ”„ Detected change in all_processed_posts.json. Processing new posts...\n",
      "\n",
      "ðŸ”¹ Found 0 new unclustered posts.\n",
      "ðŸ”¹ No new posts to process. Waiting for updates...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "\n",
      "ðŸ”„ Detected change in all_processed_posts.json. Processing new posts...\n",
      "\n",
      "ðŸ”¹ Found 0 new unclustered posts.\n",
      "ðŸ”¹ No new posts to process. Waiting for updates...\n",
      "â³ No changes detected. Waiting...\n",
      "\n",
      "ðŸ”„ Detected change in all_processed_posts.json. Processing new posts...\n",
      "\n",
      "ðŸ”¹ Found 2 new unclustered posts.\n",
      "post added succefully\n",
      "post added succefully\n",
      "âœ… ALL Data successfully updated in clustered_disasters.json\n",
      "â³ No changes detected. Waiting...\n",
      "\n",
      "ðŸ”„ Detected change in all_processed_posts.json. Processing new posts...\n",
      "\n",
      "ðŸ”¹ Found 0 new unclustered posts.\n",
      "ðŸ”¹ No new posts to process. Waiting for updates...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n",
      "â³ No changes detected. Waiting...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 316\u001b[0m\n\u001b[0;32m    313\u001b[0m last_mtime \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetmtime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_processed_posts.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Get initial modification time\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 316\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m10\u001b[39m)  \u001b[38;5;66;03m# Wait 10 seconds\u001b[39;00m\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    319\u001b[0m         current_mtime \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mgetmtime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall_processed_posts.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# File path for events.json\n",
    "events_file = \"events.json\"\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "# Define disaster keywords\n",
    "disaster_keywords = {\n",
    "    \"Earthquake\": [\"earthquake\", \"seismic\", \"magnitude\", \"tremor\", \"fault line\", \"aftershock\"],\n",
    "    \"Flood\": [\"flood\", \"flash flood\", \"heavy rain\", \"overflow\", \"dam break\", \"inundation\", \"flooding\"],\n",
    "    \"Hurricane\": [\"hurricane\", \"typhoon\", \"cyclone\", \"storm surge\", \"tropical storm\", \"storm\", \"thunderstorm\"],\n",
    "    \"Tornado\": [\"tornado\", \"twister\", \"funnel cloud\", \"whirlwind\"],\n",
    "    \"Industrial Accident\": [\"explosion\", \"chemical spill\", \"factory fire\", \"toxic leak\", \"gas leak\"],\n",
    "    \"Road Accident\": [\"road accident\", \"car crash\", \"traffic\", \"collision\", \"road mishap\", \"vehicle overturned\", \"Highway\"]\n",
    "}\n",
    "\n",
    "# Function to classify disasters\n",
    "def classify_disaster(title, selftext):\n",
    "    text = f\"{title} {selftext}\".lower()\n",
    "    for disaster, keywords in disaster_keywords.items():\n",
    "        if any(re.search(rf\"\\b{word}\\b\", text) for word in keywords):\n",
    "            return disaster  \n",
    "    return \"Other\"\n",
    "\n",
    "# Function to extract location\n",
    "def extract_location(text):\n",
    "    doc = nlp(text)\n",
    "    locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
    "    return locations[0] if locations else \"Unknown\"\n",
    "\n",
    "# Function to process new posts\n",
    "def process_new_posts():\n",
    "    try:\n",
    "        # Load all processed posts\n",
    "        with open(\"all_processed_posts.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            all_processed_posts = json.load(file)\n",
    "        \n",
    "        # Load clustered disasters\n",
    "        with open(\"clustered_disasters.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "            clustered_disasters = json.load(file)\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        clustered_df = pd.DataFrame(clustered_disasters)\n",
    "        \n",
    "        # Extract URLs from clustered disasters\n",
    "        clustered_urls = {post[\"url\"] for post in clustered_disasters}\n",
    "        \n",
    "        # Filter new posts\n",
    "        new_posts = [post for post in all_processed_posts if post[\"url\"] not in clustered_urls]\n",
    "\n",
    "        # Save new unclustered posts\n",
    "        with open(\"new_unclustered_posts.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(new_posts, file, indent=4)\n",
    "\n",
    "        print(f\"ðŸ”¹ Found {len(new_posts)} new unclustered posts.\")\n",
    "\n",
    "        # âœ… If no new posts, return instead of stopping\n",
    "        if not new_posts:\n",
    "            print(\"ðŸ”¹ No new posts to process. Waiting for updates...\")\n",
    "            return  \n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(new_posts)\n",
    "\n",
    "        # Apply disaster classification\n",
    "        df[\"disaster_type\"] = df.apply(lambda row: classify_disaster(row[\"title\"], row[\"selftext\"]), axis=1)\n",
    "\n",
    "        # Apply location extraction\n",
    "        df[\"location\"] = df.apply(lambda row: extract_location(row[\"title\"] + \" \" + row[\"selftext\"]), axis=1)\n",
    "\n",
    "        # Create event_name field\n",
    "        df[\"event_name\"] = df.apply(lambda row: f\"{row['disaster_type']} - {row['location']}\" \n",
    "                                    if row[\"location\"] != \"Unknown\" else f\"{row['disaster_type']} - Unknown\", axis=1)\n",
    "\n",
    "        # print(\"created event_name field in df\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Initialize sub-clusters\n",
    "        df[\"is_first_post\"] = False\n",
    "        \n",
    "        # Process sub-clustering\n",
    "        for index, row in df.iterrows():\n",
    "            # Load existing events\n",
    "            with open(\"events.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "                existing_events = json.load(file)\n",
    "            # print(\"loading events.json\")\n",
    "            \n",
    "            # Extract existing event names\n",
    "            existing_event_names = {re.sub(r\" - Cluster \\d+$\", \"\", event[\"event_name\"]) for event in existing_events}\n",
    "            # print(\"extracting existing event names...\")\n",
    "            # print(existing_event_names)\n",
    "\n",
    "            # Load clustered disasters\n",
    "            with open(\"clustered_disasters.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "                new_clustered_disasters = json.load(file)\n",
    "            # print(\"Loaded the cluster disaster file inside the loop\")\n",
    "        \n",
    "            # Convert to DataFrame\n",
    "            new_clustered_df = pd.DataFrame(new_clustered_disasters)\n",
    "            # print(\"Converting loaded file to dataframe\")\n",
    "            \n",
    "            if row[\"event_name\"] in existing_event_names:\n",
    "                # print(\"if event_name exists in events.json\")\n",
    "                base_event_name = row[\"event_name\"]\n",
    "                # print(\"set base_event_name\")\n",
    "\n",
    "                \n",
    "                \n",
    "                # Check if new_clustered_df is empty\n",
    "                if new_clustered_df.empty:\n",
    "                    # print(\"Cluster is empty\")\n",
    "                    df.at[index, \"event_name\"] = f\"{row['event_name']} - Cluster 0\"\n",
    "                    df.at[index, \"is_first_post\"] = True\n",
    "                    df.at[index, \"sub_cluster\"] = 0\n",
    "                    continue  # Move to the next iteration of the loop\n",
    "                \n",
    "                # Find matching clustered posts\n",
    "                matching_posts = new_clustered_df[\n",
    "                    new_clustered_df[\"event_name\"].str.startswith(base_event_name)\n",
    "                ].copy()\n",
    "                # print(\"finding matching posts with same base_event_name\")\n",
    "                \n",
    "                if not matching_posts.empty:\n",
    "                    print(f\"ðŸ”¹ Found matching clustered posts for '{base_event_name}'\")\n",
    "\n",
    "                    # Extract text features\n",
    "                    matching_posts[\"combined_text\"] = matching_posts[\"title\"] + \" \" + matching_posts[\"selftext\"]\n",
    "                    current_post_text = row[\"title\"] + \" \" + row[\"selftext\"]\n",
    "\n",
    "                    # Compute TF-IDF vectors\n",
    "                    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
    "                    X = vectorizer.fit_transform(matching_posts[\"combined_text\"].tolist() + [current_post_text])\n",
    "\n",
    "                    # Compute cosine similarity\n",
    "                    similarities = cosine_similarity(X[-1:], X[:-1]).flatten()\n",
    "\n",
    "                    # Find best match\n",
    "                    max_similarity_index = similarities.argmax()\n",
    "                    max_similarity_score = similarities[max_similarity_index]\n",
    "\n",
    "                    # Find the max similarity score\n",
    "                    max_similarity_score = similarities.max()\n",
    "                    \n",
    "                    # Get all indices where similarity is equal to max\n",
    "                    max_similarity_indices = [i for i, score in enumerate(similarities) if score == max_similarity_score]\n",
    "                    \n",
    "                    if max_similarity_score > 0.5:\n",
    "\n",
    "                        # Get all sub-clusters with the max similarity score\n",
    "                        possible_sub_clusters = [int(sub_cluster) for sub_cluster in matching_posts.iloc[max_similarity_indices][\"sub_cluster\"].unique()]\n",
    "                        \n",
    "                        # Construct all possible event names\n",
    "                        possible_event_names = [f\"{base_event_name} - Cluster {sub_cluster}\" for sub_cluster in possible_sub_clusters]\n",
    "                    \n",
    "                        print(f\"âœ… Possible sub-clusters: {possible_sub_clusters}\")\n",
    "                        print(f\"âœ… Possible event names: {possible_event_names}\")\n",
    "                    \n",
    "                        # Check for first posts in new_clustered_df with any possible event name\n",
    "                        first_posts = new_clustered_df[\n",
    "                            (new_clustered_df[\"event_name\"].isin(possible_event_names)) & \n",
    "                            (new_clustered_df[\"is_first_post\"] == True)\n",
    "                        ]\n",
    "                    \n",
    "                        if not first_posts.empty:\n",
    "                            print(f\"ðŸŸ¢ Found first posts in: {first_posts['event_name'].tolist()}\")\n",
    "                    \n",
    "                            # Extract `created_utc` of all first posts\n",
    "                            first_posts_utc = first_posts[\"created_utc\"].tolist()\n",
    "                            current_post_utc = row[\"created_utc\"]\n",
    "                    \n",
    "                            # Check if current post lies within 30 days of any first post\n",
    "                            within_30_days = [\n",
    "                                sub_cluster for sub_cluster, utc in zip(first_posts[\"sub_cluster\"], first_posts_utc)\n",
    "                                if (current_post_utc - utc) < (30 * 24 * 60 * 60)  # 30 days in seconds\n",
    "                            ]\n",
    "                    \n",
    "                            if within_30_days:\n",
    "                                assigned_sub_cluster = int(within_30_days[0])  # Assign first matching cluster\n",
    "                                print(f\"ðŸ”„ Assigned existing sub-cluster {assigned_sub_cluster} (within 30 days).\")\n",
    "                            else:\n",
    "                                assigned_sub_cluster = int(max(possible_sub_clusters) + 1)\n",
    "                                df.at[index, \"is_first_post\"] = True\n",
    "                                print(f\"ðŸ•’ More than 30 days since all first posts. Assigning new sub-cluster {assigned_sub_cluster}\")\n",
    "                    \n",
    "                        else:\n",
    "                            assigned_sub_cluster = int(max(possible_sub_clusters) + 1)\n",
    "                            df.at[index, \"is_first_post\"] = True\n",
    "                            print(f\"âš ï¸ No first posts found. Assigning new sub-cluster {assigned_sub_cluster}\")\n",
    "                    \n",
    "                        \n",
    "\n",
    "\n",
    "                        \n",
    "                    else:\n",
    "                        assigned_sub_cluster = int(matching_posts[\"sub_cluster\"].max() + 1) if not matching_posts[\"sub_cluster\"].isna().all() else 0\n",
    "                        print(f\"âž• New sub-cluster {assigned_sub_cluster} assigned\")\n",
    "                        df.at[index, \"is_first_post\"] = True\n",
    "                        \n",
    "\n",
    "                    # Assign sub-cluster\n",
    "                    df.at[index, \"sub_cluster\"] = int(assigned_sub_cluster)\n",
    "                    df.at[index, \"event_name\"] = f\"{row['event_name']} - Cluster {assigned_sub_cluster}\"\n",
    "                    # Load existing events\n",
    "                    if os.path.exists(events_file):\n",
    "                        with open(events_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                            try:\n",
    "                                events_list = json.load(f)\n",
    "                            except json.JSONDecodeError:\n",
    "                                events_list = []  # Handle empty or corrupted JSON\n",
    "                    else:\n",
    "                        events_list = []\n",
    "                        \n",
    "                    # New event object\n",
    "                    new_event_name = {\"event_name\": df.at[index, \"event_name\"]}\n",
    "                        \n",
    "                    # Append only if it doesn't already exist\n",
    "                    if new_event_name not in events_list:\n",
    "                        events_list.append(new_event_name)\n",
    "                        \n",
    "                    # Save updated event list\n",
    "                    with open(events_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(events_list, f, indent=4, ensure_ascii=False)\n",
    "                    \n",
    "                else:\n",
    "                    df.at[index, \"sub_cluster\"] = 0  \n",
    "                    df.at[index, \"event_name\"] = f\"{row['event_name']} - Cluster 0\"\n",
    "                    df.at[index, \"is_first_post\"] = True\n",
    "                    # Load existing events\n",
    "                    if os.path.exists(events_file):\n",
    "                        with open(events_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                            try:\n",
    "                                events_list = json.load(f)\n",
    "                            except json.JSONDecodeError:\n",
    "                                events_list = []  # Handle empty or corrupted JSON\n",
    "                    else:\n",
    "                        events_list = []\n",
    "                        \n",
    "                    # New event object\n",
    "                    new_event_name = {\"event_name\": df.at[index, \"event_name\"]}\n",
    "                        \n",
    "                    # Append only if it doesn't already exist\n",
    "                    if new_event_name not in events_list:\n",
    "                        events_list.append(new_event_name)\n",
    "                        \n",
    "                    # Save updated event list\n",
    "                    with open(events_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                        json.dump(events_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            else:\n",
    "                # print(\"matching event name not found\")\n",
    "                # print(\"event name of  current post -\")\n",
    "                # print(df.at[index, \"event_name\"])\n",
    "                df.at[index, \"event_name\"] = f\"{row['event_name']} - Cluster 0\"\n",
    "                df.at[index, \"is_first_post\"] = True\n",
    "                df.at[index, \"sub_cluster\"] = 0\n",
    "                # print(\"updated event name of the current post -\")\n",
    "                # print(df.at[index, \"event_name\"])\n",
    "                # Load existing events\n",
    "                if os.path.exists(events_file):\n",
    "                    with open(events_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                        try:\n",
    "                            events_list = json.load(f)\n",
    "                        except json.JSONDecodeError:\n",
    "                            events_list = []  # Handle empty or corrupted JSON\n",
    "                else:\n",
    "                    events_list = []\n",
    "                        \n",
    "                # New event object\n",
    "                new_event_name = {\"event_name\": df.at[index, \"event_name\"]}\n",
    "                # print(\"new event name -\")\n",
    "                # print(new_event_name)\n",
    "                \n",
    "                # Append only if it doesn't already exist\n",
    "                if new_event_name not in events_list:\n",
    "                    events_list.append(new_event_name)\n",
    "                \n",
    "                # Save updated event list\n",
    "                with open(events_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    json.dump(events_list, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            \n",
    "            # **Update clustered_disasters.json after processing the current row**\n",
    "            current_row_df = df.loc[[index]]  # Convert the current row to DataFrame\n",
    "        \n",
    "            updated_clustered_df = pd.concat([new_clustered_df, current_row_df], ignore_index=True)\n",
    "        \n",
    "            json_data = updated_clustered_df.to_json(orient=\"records\", indent=4)\n",
    "            parsed_json = json.loads(json_data)\n",
    "        \n",
    "            with open(\"clustered_disasters.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(parsed_json, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "            print(\"post added succefully\")\n",
    "                \n",
    "    \n",
    "        print(\"âœ… ALL Data successfully updated in clustered_disasters.json\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error occurred: {e}\")\n",
    "\n",
    "# Infinite loop to check file modification time\n",
    "last_mtime = os.path.getmtime(\"all_processed_posts.json\")  # Get initial modification time\n",
    "\n",
    "while True:\n",
    "    time.sleep(10)  # Wait 10 seconds\n",
    "\n",
    "    try:\n",
    "        current_mtime = os.path.getmtime(\"all_processed_posts.json\")\n",
    "            \n",
    "        if current_mtime != last_mtime:\n",
    "            print(\"\\nðŸ”„ Detected change in all_processed_posts.json. Processing new posts...\\n\")\n",
    "            process_new_posts()\n",
    "            last_mtime = current_mtime  # Update last modification time\n",
    "\n",
    "        else:\n",
    "            print(\"â³ No changes detected. Waiting...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error monitoring file: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
